# Multilayer GRU with Batch Normalization on Karpathy Name Dataset

This project implements a **character-level language model** trained on [Andrej Karpathy's Names Dataset](https://github.com/karpathy/char-rnn), with a strong emphasis on **manual architecture design** and **efficient computation** using only the essential components of PyTorch.

## ðŸš€ Key Features

- âœ… **Multilayer GRU (Gated Recurrent Unit)** network built from scratch
- âœ… **Batch Normalization inside GRU cells**, custom-implemented and deeply integrated
- âœ… Efficient **vectorized operations** for fast forward and backward passes
- âœ… Manual loop-based training with explicit tensor operations
- âœ… Uses only **PyTorch Tensors and Autograd** â€“ *no torch.nn, no torch.optim, no high-level API*
- âœ… Trained on the real-world character-level dataset of names for sequence modeling

## ðŸ“š Dataset

The dataset consists of thousands of human names from multiple cultures, allowing the model to learn the structure of names and generate new, realistic samples.

## ðŸŽ¯ Objectives

- Build deep learning components manually to understand their inner workings
- Explore the behavior and benefits of **batch normalization inside recurrent networks**
- Learn to handle multi-layer recurrent models and custom training pipelines using only PyTorch low-level APIs
- Compare and experiment with different architectural designs from the ground up

## ðŸ§  Why This Project Matters

This project is a stepping stone toward truly mastering deep learning. Instead of relying on ready-made building blocks, this codebase **forces an understanding** of what actually happens under the hood when we train recurrent models.

It's also a great preparation for exploring more advanced areas like transformers, optimization tricks, and performance engineering.

## ðŸ“‚ Folder Structure

